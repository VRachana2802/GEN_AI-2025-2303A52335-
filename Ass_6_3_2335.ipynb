{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbYY3YpkZqPNs5fdcOY5bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VRachana2802/GEN_AI-2025-2303A52335-/blob/main/Ass_6_3_2335.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHIaXEgm9j8E",
        "outputId": "36b71d73-21c7-42e6-a0ab-f4400b80f3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - loss: 153812.8125 - rmse: 392.1703 - val_loss: 159937.3125 - val_rmse: 399.9216\n",
            "Epoch 2/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 156073.2500 - rmse: 395.0247 - val_loss: 159693.1406 - val_rmse: 399.6162\n",
            "Epoch 3/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 148964.6250 - rmse: 385.9304 - val_loss: 159340.0469 - val_rmse: 399.1742\n",
            "Epoch 4/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 153188.4531 - rmse: 391.3856 - val_loss: 158855.4375 - val_rmse: 398.5667\n",
            "Epoch 5/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 151025.5312 - rmse: 388.6093 - val_loss: 158224.9062 - val_rmse: 397.7749\n",
            "Epoch 6/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 151493.2812 - rmse: 389.1333 - val_loss: 157447.6562 - val_rmse: 396.7968\n",
            "Epoch 7/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 155691.8594 - rmse: 394.4110 - val_loss: 156542.2031 - val_rmse: 395.6541\n",
            "Epoch 8/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 149796.9844 - rmse: 387.0203 - val_loss: 155425.7031 - val_rmse: 394.2407\n",
            "Epoch 9/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 151836.0781 - rmse: 389.5998 - val_loss: 154045.3750 - val_rmse: 392.4861\n",
            "Epoch 10/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 150692.4844 - rmse: 388.1556 - val_loss: 152346.5625 - val_rmse: 390.3160\n",
            "Epoch 11/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 144799.4062 - rmse: 380.5081 - val_loss: 150211.3594 - val_rmse: 387.5711\n",
            "Epoch 12/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 145800.0000 - rmse: 381.8105 - val_loss: 147606.1562 - val_rmse: 384.1955\n",
            "Epoch 13/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 143620.5000 - rmse: 378.9379 - val_loss: 144510.3906 - val_rmse: 380.1452\n",
            "Epoch 14/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 139227.5781 - rmse: 373.1063 - val_loss: 140933.9531 - val_rmse: 375.4117\n",
            "Epoch 15/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 133334.1719 - rmse: 365.1165 - val_loss: 136701.1406 - val_rmse: 369.7312\n",
            "Epoch 16/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 132302.6875 - rmse: 363.6718 - val_loss: 131822.9062 - val_rmse: 363.0742\n",
            "Epoch 17/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 127713.3047 - rmse: 357.3309 - val_loss: 126147.9062 - val_rmse: 355.1731\n",
            "Epoch 18/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 124685.2109 - rmse: 353.0438 - val_loss: 119733.8438 - val_rmse: 346.0258\n",
            "Epoch 19/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 112802.0859 - rmse: 335.8506 - val_loss: 112519.6406 - val_rmse: 335.4395\n",
            "Epoch 20/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 104825.1406 - rmse: 323.7524 - val_loss: 104553.1094 - val_rmse: 323.3467\n",
            "Epoch 21/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 100894.1250 - rmse: 317.5916 - val_loss: 96088.7969 - val_rmse: 309.9819\n",
            "Epoch 22/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 94594.4844 - rmse: 307.4513 - val_loss: 86828.8672 - val_rmse: 294.6674\n",
            "Epoch 23/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 81886.6094 - rmse: 286.1341 - val_loss: 76957.4297 - val_rmse: 277.4120\n",
            "Epoch 24/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 73792.1562 - rmse: 271.5060 - val_loss: 66819.1875 - val_rmse: 258.4941\n",
            "Epoch 25/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 62081.3633 - rmse: 249.1471 - val_loss: 56406.2461 - val_rmse: 237.5000\n",
            "Epoch 26/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 55053.1523 - rmse: 234.5407 - val_loss: 46658.9062 - val_rmse: 216.0067\n",
            "Epoch 27/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 43341.2656 - rmse: 208.1748 - val_loss: 36845.4492 - val_rmse: 191.9517\n",
            "Epoch 28/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 34925.6562 - rmse: 186.8546 - val_loss: 28210.8457 - val_rmse: 167.9608\n",
            "Epoch 29/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 27831.4707 - rmse: 166.6832 - val_loss: 20640.7090 - val_rmse: 143.6687\n",
            "Epoch 30/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 19744.6758 - rmse: 140.4632 - val_loss: 14376.4688 - val_rmse: 119.9019\n",
            "Epoch 31/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13900.5059 - rmse: 117.8756 - val_loss: 10010.2471 - val_rmse: 100.0512\n",
            "Epoch 32/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10172.3594 - rmse: 100.7974 - val_loss: 6927.2002 - val_rmse: 83.2298\n",
            "Epoch 33/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7143.7905 - rmse: 84.4617 - val_loss: 5060.5674 - val_rmse: 71.1377\n",
            "Epoch 34/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5089.4312 - rmse: 71.3239 - val_loss: 3826.7769 - val_rmse: 61.8609\n",
            "Epoch 35/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4089.1260 - rmse: 63.9284 - val_loss: 2975.0693 - val_rmse: 54.5442\n",
            "Epoch 36/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3387.6064 - rmse: 58.1381 - val_loss: 2538.8650 - val_rmse: 50.3871\n",
            "Epoch 37/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2767.3230 - rmse: 52.5672 - val_loss: 2151.4502 - val_rmse: 46.3837\n",
            "Epoch 38/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2284.1299 - rmse: 47.7876 - val_loss: 1951.9230 - val_rmse: 44.1806\n",
            "Epoch 39/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2029.6215 - rmse: 45.0388 - val_loss: 1814.2467 - val_rmse: 42.5940\n",
            "Epoch 40/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1808.9211 - rmse: 42.5255 - val_loss: 1665.1343 - val_rmse: 40.8061\n",
            "Epoch 41/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1681.0029 - rmse: 40.9859 - val_loss: 1490.8231 - val_rmse: 38.6112\n",
            "Epoch 42/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1539.2224 - rmse: 39.2230 - val_loss: 1332.6870 - val_rmse: 36.5060\n",
            "Epoch 43/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1368.7979 - rmse: 36.9869 - val_loss: 1248.0068 - val_rmse: 35.3271\n",
            "Epoch 44/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1257.3904 - rmse: 35.4439 - val_loss: 1137.8425 - val_rmse: 33.7319\n",
            "Epoch 45/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1166.2903 - rmse: 34.1491 - val_loss: 1033.9216 - val_rmse: 32.1547\n",
            "Epoch 46/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1046.2161 - rmse: 32.3414 - val_loss: 1050.7217 - val_rmse: 32.4148\n",
            "Epoch 47/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 997.7662 - rmse: 31.5691 - val_loss: 866.6052 - val_rmse: 29.4382\n",
            "Epoch 48/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 857.0676 - rmse: 29.2473 - val_loss: 827.4601 - val_rmse: 28.7656\n",
            "Epoch 49/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 789.6079 - rmse: 28.0969 - val_loss: 749.0023 - val_rmse: 27.3679\n",
            "Epoch 50/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 669.3442 - rmse: 25.8355 - val_loss: 809.7110 - val_rmse: 28.4554\n",
            "Epoch 51/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 739.1700 - rmse: 27.1846 - val_loss: 660.8074 - val_rmse: 25.7062\n",
            "Epoch 52/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 621.2269 - rmse: 24.8596 - val_loss: 595.8287 - val_rmse: 24.4096\n",
            "Epoch 53/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 640.5104 - rmse: 25.3070 - val_loss: 683.1795 - val_rmse: 26.1377\n",
            "Epoch 54/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 607.3096 - rmse: 24.6314 - val_loss: 602.3242 - val_rmse: 24.5423\n",
            "Epoch 55/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 562.7014 - rmse: 23.7079 - val_loss: 497.6648 - val_rmse: 22.3084\n",
            "Epoch 56/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 481.5897 - rmse: 21.9373 - val_loss: 464.1524 - val_rmse: 21.5442\n",
            "Epoch 57/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 436.1471 - rmse: 20.8655 - val_loss: 487.5312 - val_rmse: 22.0801\n",
            "Epoch 58/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 422.5333 - rmse: 20.5433 - val_loss: 412.5821 - val_rmse: 20.3121\n",
            "Epoch 59/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 423.7788 - rmse: 20.5691 - val_loss: 411.6919 - val_rmse: 20.2902\n",
            "Epoch 60/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 367.7889 - rmse: 19.1540 - val_loss: 388.9583 - val_rmse: 19.7220\n",
            "Epoch 61/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 391.1958 - rmse: 19.7593 - val_loss: 340.3547 - val_rmse: 18.4487\n",
            "Epoch 62/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 427.2236 - rmse: 20.6501 - val_loss: 350.7659 - val_rmse: 18.7287\n",
            "Epoch 63/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 308.0169 - rmse: 17.5474 - val_loss: 356.8983 - val_rmse: 18.8918\n",
            "Epoch 64/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 320.6312 - rmse: 17.9033 - val_loss: 302.7052 - val_rmse: 17.3984\n",
            "Epoch 65/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 323.4982 - rmse: 17.9496 - val_loss: 293.1836 - val_rmse: 17.1226\n",
            "Epoch 66/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 283.3337 - rmse: 16.8285 - val_loss: 294.4612 - val_rmse: 17.1599\n",
            "Epoch 67/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 329.0697 - rmse: 18.1225 - val_loss: 270.5457 - val_rmse: 16.4483\n",
            "Epoch 68/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 268.6727 - rmse: 16.3897 - val_loss: 327.7525 - val_rmse: 18.1039\n",
            "Epoch 69/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 306.8830 - rmse: 17.4785 - val_loss: 290.8473 - val_rmse: 17.0542\n",
            "Epoch 70/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 269.1968 - rmse: 16.4001 - val_loss: 228.1741 - val_rmse: 15.1054\n",
            "Epoch 71/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 252.8078 - rmse: 15.8907 - val_loss: 254.6869 - val_rmse: 15.9589\n",
            "Epoch 72/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 247.0833 - rmse: 15.7129 - val_loss: 293.2394 - val_rmse: 17.1242\n",
            "Epoch 73/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 228.6258 - rmse: 15.1088 - val_loss: 222.3355 - val_rmse: 14.9109\n",
            "Epoch 74/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 204.2595 - rmse: 14.2658 - val_loss: 202.8300 - val_rmse: 14.2418\n",
            "Epoch 75/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 227.5471 - rmse: 15.0675 - val_loss: 203.8023 - val_rmse: 14.2759\n",
            "Epoch 76/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 223.2755 - rmse: 14.9216 - val_loss: 192.9360 - val_rmse: 13.8901\n",
            "Epoch 77/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 193.6240 - rmse: 13.9009 - val_loss: 187.1042 - val_rmse: 13.6786\n",
            "Epoch 78/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 191.1113 - rmse: 13.8165 - val_loss: 179.2463 - val_rmse: 13.3883\n",
            "Epoch 79/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 205.0487 - rmse: 14.2987 - val_loss: 178.8106 - val_rmse: 13.3720\n",
            "Epoch 80/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 176.3798 - rmse: 13.2666 - val_loss: 204.5513 - val_rmse: 14.3021\n",
            "Epoch 81/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 200.4690 - rmse: 14.1456 - val_loss: 180.3079 - val_rmse: 13.4279\n",
            "Epoch 82/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 197.6575 - rmse: 14.0455 - val_loss: 164.4210 - val_rmse: 12.8227\n",
            "Epoch 83/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 162.9470 - rmse: 12.7427 - val_loss: 212.1507 - val_rmse: 14.5654\n",
            "Epoch 84/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 176.2237 - rmse: 13.2663 - val_loss: 323.5717 - val_rmse: 17.9881\n",
            "Epoch 85/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 190.7306 - rmse: 13.7748 - val_loss: 164.4667 - val_rmse: 12.8245\n",
            "Epoch 86/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 178.3171 - rmse: 13.3448 - val_loss: 159.8774 - val_rmse: 12.6443\n",
            "Epoch 87/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 206.7716 - rmse: 14.3594 - val_loss: 180.9282 - val_rmse: 13.4510\n",
            "Epoch 88/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 175.6782 - rmse: 13.2516 - val_loss: 156.3946 - val_rmse: 12.5058\n",
            "Epoch 89/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 160.4227 - rmse: 12.6642 - val_loss: 148.4678 - val_rmse: 12.1847\n",
            "Epoch 90/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 184.7354 - rmse: 13.5632 - val_loss: 147.7303 - val_rmse: 12.1544\n",
            "Epoch 91/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 161.9392 - rmse: 12.6936 - val_loss: 141.8812 - val_rmse: 11.9114\n",
            "Epoch 92/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 157.0294 - rmse: 12.5189 - val_loss: 190.4386 - val_rmse: 13.7999\n",
            "Epoch 93/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 148.1471 - rmse: 12.1650 - val_loss: 174.7954 - val_rmse: 13.2210\n",
            "Epoch 94/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 158.2590 - rmse: 12.5751 - val_loss: 267.5457 - val_rmse: 16.3568\n",
            "Epoch 95/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 162.0109 - rmse: 12.7262 - val_loss: 141.8564 - val_rmse: 11.9103\n",
            "Epoch 96/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 140.0346 - rmse: 11.8289 - val_loss: 351.0508 - val_rmse: 18.7363\n",
            "Epoch 97/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 208.7095 - rmse: 14.3345 - val_loss: 320.4871 - val_rmse: 17.9022\n",
            "Epoch 98/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 179.7689 - rmse: 13.3640 - val_loss: 169.1212 - val_rmse: 13.0047\n",
            "Epoch 99/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 153.3650 - rmse: 12.3769 - val_loss: 217.4333 - val_rmse: 14.7456\n",
            "Epoch 100/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 149.4020 - rmse: 12.2094 - val_loss: 194.3402 - val_rmse: 13.9406\n",
            "Epoch 101/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 194.9612 - rmse: 13.9450 - val_loss: 133.2150 - val_rmse: 11.5419\n",
            "Epoch 102/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 145.4510 - rmse: 12.0162 - val_loss: 142.6420 - val_rmse: 11.9433\n",
            "Epoch 103/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 132.1255 - rmse: 11.4655 - val_loss: 137.5142 - val_rmse: 11.7266\n",
            "Epoch 104/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 154.0111 - rmse: 12.3908 - val_loss: 173.7155 - val_rmse: 13.1801\n",
            "Epoch 105/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 171.3098 - rmse: 13.0706 - val_loss: 127.7997 - val_rmse: 11.3049\n",
            "Epoch 106/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 144.9141 - rmse: 12.0195 - val_loss: 391.2971 - val_rmse: 19.7812\n",
            "Epoch 107/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 209.7566 - rmse: 14.2758 - val_loss: 132.0406 - val_rmse: 11.4909\n",
            "Epoch 108/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 148.0262 - rmse: 12.1581 - val_loss: 165.4985 - val_rmse: 12.8646\n",
            "Epoch 109/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 134.4413 - rmse: 11.5758 - val_loss: 124.1928 - val_rmse: 11.1442\n",
            "Epoch 110/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 148.5441 - rmse: 12.1609 - val_loss: 158.5819 - val_rmse: 12.5929\n",
            "Epoch 111/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 139.1703 - rmse: 11.7767 - val_loss: 124.7403 - val_rmse: 11.1687\n",
            "Epoch 112/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 124.2970 - rmse: 11.1406 - val_loss: 248.5490 - val_rmse: 15.7654\n",
            "Epoch 113/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 151.0206 - rmse: 12.2072 - val_loss: 146.4485 - val_rmse: 12.1016\n",
            "Epoch 114/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 146.2852 - rmse: 12.0846 - val_loss: 241.4128 - val_rmse: 15.5375\n",
            "Epoch 115/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 171.0023 - rmse: 13.0273 - val_loss: 279.6007 - val_rmse: 16.7213\n",
            "Epoch 116/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 175.2765 - rmse: 13.1814 - val_loss: 122.2290 - val_rmse: 11.0557\n",
            "Epoch 117/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 144.2556 - rmse: 11.9425 - val_loss: 121.2105 - val_rmse: 11.0096\n",
            "Epoch 118/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 123.2473 - rmse: 11.0791 - val_loss: 124.5442 - val_rmse: 11.1599\n",
            "Epoch 119/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 132.4713 - rmse: 11.4947 - val_loss: 139.3803 - val_rmse: 11.8059\n",
            "Epoch 120/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 118.5150 - rmse: 10.8813 - val_loss: 472.0859 - val_rmse: 21.7275\n",
            "Epoch 121/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 183.4528 - rmse: 13.4220 - val_loss: 120.3325 - val_rmse: 10.9696\n",
            "Epoch 122/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 151.2321 - rmse: 12.2563 - val_loss: 121.5038 - val_rmse: 11.0229\n",
            "Epoch 123/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 129.1966 - rmse: 11.3549 - val_loss: 129.3091 - val_rmse: 11.3714\n",
            "Epoch 124/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 121.8465 - rmse: 11.0332 - val_loss: 197.2492 - val_rmse: 14.0445\n",
            "Epoch 125/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 161.7534 - rmse: 12.6997 - val_loss: 121.0842 - val_rmse: 11.0038\n",
            "Epoch 126/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 178.6059 - rmse: 13.3063 - val_loss: 119.8709 - val_rmse: 10.9486\n",
            "Epoch 127/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 125.3687 - rmse: 11.1735 - val_loss: 118.3234 - val_rmse: 10.8777\n",
            "Epoch 128/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 119.1942 - rmse: 10.9057 - val_loss: 262.5598 - val_rmse: 16.2037\n",
            "Epoch 129/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 156.5079 - rmse: 12.4549 - val_loss: 243.2694 - val_rmse: 15.5971\n",
            "Epoch 130/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 165.9276 - rmse: 12.8140 - val_loss: 164.1036 - val_rmse: 12.8103\n",
            "Epoch 131/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 119.3363 - rmse: 10.8956 - val_loss: 220.3982 - val_rmse: 14.8458\n",
            "Epoch 132/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 214.6249 - rmse: 14.5840 - val_loss: 169.9851 - val_rmse: 13.0378\n",
            "Epoch 133/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 138.9161 - rmse: 11.7764 - val_loss: 117.4392 - val_rmse: 10.8369\n",
            "Epoch 134/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 122.7461 - rmse: 11.0104 - val_loss: 117.9677 - val_rmse: 10.8613\n",
            "Epoch 135/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 115.8761 - rmse: 10.7442 - val_loss: 145.5684 - val_rmse: 12.0652\n",
            "Epoch 136/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 138.7673 - rmse: 11.7644 - val_loss: 147.7430 - val_rmse: 12.1550\n",
            "Epoch 137/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 153.6472 - rmse: 12.3905 - val_loss: 125.4206 - val_rmse: 11.1991\n",
            "Epoch 138/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 126.6070 - rmse: 11.2348 - val_loss: 167.4654 - val_rmse: 12.9408\n",
            "Epoch 139/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 137.5356 - rmse: 11.7243 - val_loss: 118.4302 - val_rmse: 10.8826\n",
            "Epoch 140/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 115.1979 - rmse: 10.7130 - val_loss: 219.0363 - val_rmse: 14.7999\n",
            "Epoch 141/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 128.6136 - rmse: 11.3215 - val_loss: 127.5383 - val_rmse: 11.2933\n",
            "Epoch 142/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 151.6267 - rmse: 12.2855 - val_loss: 189.0881 - val_rmse: 13.7509\n",
            "Epoch 143/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 168.5354 - rmse: 12.9579 - val_loss: 241.7755 - val_rmse: 15.5491\n",
            "Epoch 144/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 195.6392 - rmse: 13.8766 - val_loss: 121.6157 - val_rmse: 11.0279\n",
            "Epoch 145/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 118.6576 - rmse: 10.8596 - val_loss: 120.6622 - val_rmse: 10.9846\n",
            "Epoch 146/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 125.6228 - rmse: 11.1886 - val_loss: 138.1337 - val_rmse: 11.7530\n",
            "Epoch 147/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 145.1189 - rmse: 12.0223 - val_loss: 164.5349 - val_rmse: 12.8271\n",
            "Epoch 148/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 132.5883 - rmse: 11.5053 - val_loss: 200.4648 - val_rmse: 14.1586\n",
            "Epoch 149/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 166.2876 - rmse: 12.8883 - val_loss: 123.0807 - val_rmse: 11.0942\n",
            "Epoch 150/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 121.1283 - rmse: 10.9847 - val_loss: 216.9220 - val_rmse: 14.7283\n",
            "Epoch 151/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 140.4924 - rmse: 11.8334 - val_loss: 121.8565 - val_rmse: 11.0389\n",
            "Epoch 152/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 138.7956 - rmse: 11.7724 - val_loss: 137.8024 - val_rmse: 11.7389\n",
            "Epoch 153/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 129.8542 - rmse: 11.3926 - val_loss: 182.1241 - val_rmse: 13.4953\n",
            "Epoch 154/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 152.8556 - rmse: 12.3542 - val_loss: 119.3610 - val_rmse: 10.9252\n",
            "Epoch 155/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 134.1497 - rmse: 11.5803 - val_loss: 179.1571 - val_rmse: 13.3850\n",
            "Epoch 156/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 146.5227 - rmse: 12.0798 - val_loss: 155.7541 - val_rmse: 12.4801\n",
            "Epoch 157/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 150.4951 - rmse: 12.2597 - val_loss: 115.5407 - val_rmse: 10.7490\n",
            "Epoch 158/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 154.4336 - rmse: 12.4046 - val_loss: 131.8232 - val_rmse: 11.4814\n",
            "Epoch 159/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 120.3601 - rmse: 10.9584 - val_loss: 115.7757 - val_rmse: 10.7599\n",
            "Epoch 160/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 149.2514 - rmse: 12.1720 - val_loss: 118.9148 - val_rmse: 10.9048\n",
            "Epoch 161/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 117.3801 - rmse: 10.8013 - val_loss: 196.2206 - val_rmse: 14.0079\n",
            "Epoch 162/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 128.4428 - rmse: 11.3175 - val_loss: 117.9037 - val_rmse: 10.8583\n",
            "Epoch 163/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 145.8800 - rmse: 12.0096 - val_loss: 116.4174 - val_rmse: 10.7897\n",
            "Epoch 164/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 129.5819 - rmse: 11.3743 - val_loss: 118.7229 - val_rmse: 10.8960\n",
            "Epoch 165/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 128.4928 - rmse: 11.3154 - val_loss: 119.7158 - val_rmse: 10.9415\n",
            "Epoch 166/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 116.6714 - rmse: 10.7947 - val_loss: 200.4349 - val_rmse: 14.1575\n",
            "Epoch 167/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 162.2936 - rmse: 12.7201 - val_loss: 129.0798 - val_rmse: 11.3613\n",
            "Epoch 168/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 146.7551 - rmse: 12.1043 - val_loss: 205.4962 - val_rmse: 14.3351\n",
            "Epoch 169/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 150.6317 - rmse: 12.2654 - val_loss: 114.8472 - val_rmse: 10.7167\n",
            "Epoch 170/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 128.6817 - rmse: 11.3345 - val_loss: 134.0837 - val_rmse: 11.5795\n",
            "Epoch 171/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 113.5231 - rmse: 10.6254 - val_loss: 126.5748 - val_rmse: 11.2505\n",
            "Epoch 172/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 137.5592 - rmse: 11.7063 - val_loss: 157.6586 - val_rmse: 12.5562\n",
            "Epoch 173/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 127.5850 - rmse: 11.2879 - val_loss: 117.9313 - val_rmse: 10.8596\n",
            "Epoch 174/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 133.2165 - rmse: 11.5257 - val_loss: 150.5331 - val_rmse: 12.2692\n",
            "Epoch 175/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 142.5151 - rmse: 11.9354 - val_loss: 120.7770 - val_rmse: 10.9899\n",
            "Epoch 176/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 121.2523 - rmse: 10.9886 - val_loss: 137.6613 - val_rmse: 11.7329\n",
            "Epoch 177/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 123.3930 - rmse: 11.1021 - val_loss: 113.9953 - val_rmse: 10.6769\n",
            "Epoch 178/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 141.2094 - rmse: 11.8099 - val_loss: 114.5765 - val_rmse: 10.7040\n",
            "Epoch 179/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 116.0925 - rmse: 10.7425 - val_loss: 124.7796 - val_rmse: 11.1705\n",
            "Epoch 180/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 107.6124 - rmse: 10.3713 - val_loss: 130.0563 - val_rmse: 11.4042\n",
            "Epoch 181/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 144.7779 - rmse: 11.9837 - val_loss: 134.5273 - val_rmse: 11.5986\n",
            "Epoch 182/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 130.0946 - rmse: 11.3444 - val_loss: 113.5286 - val_rmse: 10.6550\n",
            "Epoch 183/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 116.4157 - rmse: 10.7564 - val_loss: 120.5946 - val_rmse: 10.9816\n",
            "Epoch 184/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 143.8576 - rmse: 11.9891 - val_loss: 130.0034 - val_rmse: 11.4019\n",
            "Epoch 185/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 120.8230 - rmse: 10.9445 - val_loss: 116.7483 - val_rmse: 10.8050\n",
            "Epoch 186/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 120.7691 - rmse: 10.9771 - val_loss: 112.1103 - val_rmse: 10.5882\n",
            "Epoch 187/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 135.7720 - rmse: 11.5706 - val_loss: 140.5698 - val_rmse: 11.8562\n",
            "Epoch 188/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 110.7377 - rmse: 10.5204 - val_loss: 136.6391 - val_rmse: 11.6893\n",
            "Epoch 189/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 147.6121 - rmse: 12.1229 - val_loss: 137.3703 - val_rmse: 11.7205\n",
            "Epoch 190/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 130.5772 - rmse: 11.4263 - val_loss: 177.1623 - val_rmse: 13.3102\n",
            "Epoch 191/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 142.1612 - rmse: 11.8962 - val_loss: 141.1707 - val_rmse: 11.8815\n",
            "Epoch 192/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 117.3368 - rmse: 10.8301 - val_loss: 120.9207 - val_rmse: 10.9964\n",
            "Epoch 193/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 126.5695 - rmse: 11.2298 - val_loss: 115.6143 - val_rmse: 10.7524\n",
            "Epoch 194/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 143.9557 - rmse: 11.9786 - val_loss: 256.7992 - val_rmse: 16.0250\n",
            "Epoch 195/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 164.1625 - rmse: 12.7522 - val_loss: 132.6324 - val_rmse: 11.5166\n",
            "Epoch 196/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 132.6029 - rmse: 11.5055 - val_loss: 170.3631 - val_rmse: 13.0523\n",
            "Epoch 197/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 121.4258 - rmse: 11.0102 - val_loss: 138.2148 - val_rmse: 11.7565\n",
            "Epoch 198/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 166.0388 - rmse: 12.8398 - val_loss: 118.4306 - val_rmse: 10.8826\n",
            "Epoch 199/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 130.2548 - rmse: 11.3861 - val_loss: 114.2206 - val_rmse: 10.6874\n",
            "Epoch 200/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 145.3480 - rmse: 12.0183 - val_loss: 120.0732 - val_rmse: 10.9578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Training Root Mean Absolute Error: 2.884811394335747\n",
            "Testing Root Mean Absolute Error: 2.944182687037451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "Predicted House Price: 399.681396484375\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import swish\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "\n",
        "X = np.random.rand(num_samples, 4)\n",
        "X[:, 0] = X[:, 0] * 10 + 1\n",
        "X[:, 1] = X[:, 1] * 4000 + 500\n",
        "X[:, 2] = X[:, 2] * 50\n",
        "X[:, 3] = X[:, 3] * 30\n",
        "\n",
        "y = (X[:, 0] * 50 + X[:, 1] * 0.05 - X[:, 2] * 1.5 - X[:, 3] * 2) + np.random.normal(0, 10, num_samples)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(18, activation=swish, input_shape=(X_train.shape[1],)),\n",
        "    Dense(26, activation=swish),\n",
        "    Dense(20, activation=swish),\n",
        "    Dense(15, activation=swish),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='mean_squared_error',\n",
        "    metrics=[keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        ")\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_test, y_test))\n",
        "model.save('housing_price_model.h5')\n",
        "\n",
        "def root_mean_absolute_error(y_true, y_pred):\n",
        "    return np.sqrt(mean_absolute_error(y_true, y_pred))\n",
        "\n",
        "train_predictions = model.predict(X_train)\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "train_rmae = root_mean_absolute_error(y_train, train_predictions)\n",
        "test_rmae = root_mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "print(f'Training Root Mean Absolute Error: {train_rmae}')\n",
        "print(f'Testing Root Mean Absolute Error: {test_rmae}')\n",
        "loaded_model = load_model('housing_price_model.h5', custom_objects={'swish': swish})\n",
        "sample = X_test[0].reshape(1, -1)\n",
        "predicted_price = loaded_model.predict(sample)\n",
        "print(f\"Predicted House Price: {predicted_price[0][0]}\")"
      ]
    }
  ]
}